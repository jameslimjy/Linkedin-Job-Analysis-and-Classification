{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "This is a web scraper I built using Selenium and Beautiful Soup to scrape information from Linkedin. From what I read online, Linkedin's API is pretty restrictive in terms of the amount of information it allows you to attain. Hence, I opted to scrape the information myself manually. Anyways, I wanted to practice building a scraper too :) \n",
    "\n",
    "I collected the following information:\n",
    "- Job title\n",
    "- Company name\n",
    "- Country\n",
    "- When the job opening was posted\n",
    "- Number of applicants\n",
    "- Job hyperlink\n",
    "- Job description\n",
    "\n",
    "Scraping Linkedin has its challenges too. Linkedin, like many other websites, uses an *infinity scroll*, meaning instead of clicking a button to navigate to the next page, you had to scroll to the bottom of the current page and wait for the next page to *append* to the existing page. This was a slight inconvenience but it was overcame with the *scroll* function. What could not be overcome, however, was Linkedin's anti-scraping feature, which allowed a user to only view the first 1000 jobs of a search. But it was alright for me as I felt that 1000 results per search was sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# chromedriver.exe must be in the same folder\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY RUN IF CHROMEDRIVER IS OUTDATED ##\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll(browser, timeout):\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to the bottom\n",
    "        browser.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        \n",
    "        # Wait for page to load\n",
    "        sleep(timeout)\n",
    "        \n",
    "        try:\n",
    "            seemorejobs_button = browser.find_element_by_xpath('//*[@id=\"main-content\"]/div/section/button').click()\n",
    "            sleep(timeout)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        # Calculate new scroll height and compare w last scroll height\n",
    "        new_height = browser.execute_script('return document.body.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "            \n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Linkedin | Create CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(jobs):\n",
    "    for job in jobs:\n",
    "        search_this_job = job\n",
    "        url = 'https://www.linkedin.com/jobs'\n",
    "        job_search_bar_xpath = '/html/body/main/section[1]/section/div[2]/section[2]/form/section[1]/input'\n",
    "        show_more_button_xpath = '/html/body/main/section[1]/section[3]/div/section/button[1]'\n",
    "\n",
    "        # Access linkedin\n",
    "        browser = webdriver.Chrome()\n",
    "        browser.get(url)\n",
    "        sleep(1)\n",
    "\n",
    "        job_search_bar = browser.find_element_by_xpath(job_search_bar_xpath)\n",
    "        job_search_bar.click()\n",
    "        job_search_bar.send_keys(search_this_job)\n",
    "        job_search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        # Scroll to bottom of infinity page\n",
    "        scroll(browser, 3) # second parameter refers to number of sec the webdriver sleeps for, incr if your machine is slow\n",
    "\n",
    "        # Soup main page and filter out the hyperlinks\n",
    "        main_soup = soup(browser.page_source, 'lxml')\n",
    "        containers =  main_soup.findAll('a', {'class': 'result-card__full-card-link'})\n",
    "        links = [i['href'] for i in containers]\n",
    "\n",
    "        # Write results to csv file\n",
    "        filename = 'linkedin_' + search_this_job + '.csv'\n",
    "        f = open(filename, 'w',  encoding=\"utf-8\")\n",
    "        headers = 'Job Title,Company Name,Country,When Posted,No. of Applicants,Hyperlink,Job Description\\n'\n",
    "        f.write(headers)\n",
    "\n",
    "        #### Souping each page ####\n",
    "\n",
    "        for num in range(len(links)):\n",
    "            print('now at job number: ' + str(num + 1) + ' of ' + str(len(links)) + ' ...')\n",
    "            try:\n",
    "                try:\n",
    "                    current_link = links[num]\n",
    "                    browser.get(current_link)\n",
    "                    current_soup = soup(browser.page_source, 'lxml')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Press Show More dropdown button\n",
    "                try:\n",
    "                    search_for_showmore_button = browser.find_element_by_xpath(show_more_button_xpath).click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # Job Hyperlink\n",
    "                job_hyperlink = links[num]\n",
    "\n",
    "                # Job title\n",
    "                try:\n",
    "                    job_title = current_soup.find('h1', {'class': 'topcard__title'}).text.replace(',', '|')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # Company name\n",
    "                try:\n",
    "                    coy_name = current_soup.find('a', {'data-tracking-control-name': 'public_jobs_topcard_org_name'}).text.replace(',', '|')\n",
    "                except:\n",
    "                    coy_name = current_soup.find('span', {'class': 'topcard__flavor'}).text.replace(',', '|')\n",
    "\n",
    "                # Country\n",
    "                country = current_soup.find('span', {'class': 'topcard__flavor topcard__flavor--bullet'}).text.replace(',', '|')\n",
    "\n",
    "                # Posted how long ago\n",
    "                try:\n",
    "                    when_posted = current_soup.find('span', {'class': 'topcard__flavor--metadata posted-time-ago__text'}).text.replace(',', '|')\n",
    "                except:\n",
    "                    when_posted = 'nil'\n",
    "\n",
    "                # Number of applicants\n",
    "                try:\n",
    "                    num_applicants = current_soup.find('span', {'class': 'topcard__flavor--metadata topcard__flavor--bullet num-applicants__caption'}).text.replace(',', '|')\n",
    "                except:\n",
    "                    num_applicants = current_soup.find('figcaption', {'class': 'num-applicants__caption'}).text\n",
    "\n",
    "                # Job Description BoW\n",
    "                job_desc_bow = current_soup.find('div', {'class': 'show-more-less-html__markup'})\n",
    "                if job_desc_bow == None:\n",
    "                    job_desc_bow = current_soup.find('div', {'class': 'description__text description__text--rich'})\n",
    "                job_desc_bow = str(job_desc_bow)\n",
    "                job_desc_bow = job_desc_bow.replace(',', '')\n",
    "\n",
    "            except:\n",
    "                print('AN ERROR WITH THIS JOB OCCURRED, CONTINUING TO NEXT JOB')\n",
    "                job_title = 'error'\n",
    "                coy_name = 'error'\n",
    "                country = 'error'\n",
    "                when_posted = 'error'\n",
    "                num_applicants = 'error'\n",
    "                job_hyperlink = 'error'\n",
    "                tokens_str = 'error'\n",
    "\n",
    "            f.write(job_title + ',' + coy_name + ',' + country + ',' + when_posted + ',' + num_applicants + ',' + job_hyperlink + ',' + str(job_desc_bow) + '\\n')\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input job searches to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now at job number: 1 of 25 ...\n",
      "now at job number: 2 of 25 ...\n",
      "now at job number: 3 of 25 ...\n",
      "now at job number: 4 of 25 ...\n",
      "now at job number: 5 of 25 ...\n",
      "now at job number: 6 of 25 ...\n",
      "now at job number: 7 of 25 ...\n",
      "now at job number: 8 of 25 ...\n",
      "now at job number: 9 of 25 ...\n",
      "now at job number: 10 of 25 ...\n",
      "now at job number: 11 of 25 ...\n",
      "now at job number: 12 of 25 ...\n",
      "now at job number: 13 of 25 ...\n",
      "now at job number: 14 of 25 ...\n",
      "now at job number: 15 of 25 ...\n",
      "now at job number: 16 of 25 ...\n",
      "now at job number: 17 of 25 ...\n",
      "now at job number: 18 of 25 ...\n",
      "now at job number: 19 of 25 ...\n",
      "now at job number: 20 of 25 ...\n",
      "now at job number: 21 of 25 ...\n",
      "now at job number: 22 of 25 ...\n",
      "now at job number: 23 of 25 ...\n",
      "now at job number: 24 of 25 ...\n",
      "now at job number: 25 of 25 ...\n"
     ]
    }
   ],
   "source": [
    "search_these_jobs = ['data science']\n",
    "linkedin_scraper(search_these_jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
